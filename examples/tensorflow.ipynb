{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConfigState example with training a model using Tensorflow\n",
    "\n",
    "This notebook presents an example of how the config-state library can be used to design a machine learning experiment that consists in training an image classification model. We show how the different components, the dataset, the model and the optimizer can be configured and modified through a config file without requiring to write code. We also show how the experiment can be saved at regular intervals and be resumed in case of interruption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Requirements\n",
    "\n",
    "The packages `tensorflow` and `tensorflow-datasets` are required for this example:\n",
    "```\n",
    "pip install tensorflow\n",
    "pip install tensorflow-datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `MLExperiment` class\n",
    "\n",
    "The `MLExperiment` class is a `ConfigState` subclass that defines the experiment consisting of training a machine learning model for image classification. It is composed of nested `ConfigState` objects that represent the different components such as `Dataset`, `Model` and `Optimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # reduce tensorflow's verbosity\n",
    "\n",
    "from examples.tensorflow.experiment import MLExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring a `MLExperiment` experiment\n",
    "The directory `examples/tensorflow/configs` contains examples of configuration files that can be used to configure an experiment. For example:\n",
    "```yaml\n",
    "dataset:\n",
    "  name: mnist\n",
    "  batch_size: 32\n",
    "model:\n",
    "  class: MultiLayerPerceptron\n",
    "  structure: [128]\n",
    "  dropout_rate: 0.2\n",
    "optimizer:\n",
    "  class: RMSprop\n",
    "  learning_rate: 0.001\n",
    "```\n",
    "We can load it and instanciate an experiment with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  batch_size: 32\n",
      "  name: mnist\n",
      "model:\n",
      "  class: MultiLayerPerceptron\n",
      "  dropout_rate: 0.2\n",
      "  input_shape: (28, 28, 1)\n",
      "  output_units: 10\n",
      "  structure: [128]\n",
      "optimizer:\n",
      "  class: RMSprop\n",
      "  epsilon: 1e-07\n",
      "  learning_rate: 0.001\n",
      "  momentum: 0.0\n",
      "  rho: 0.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "config = yaml.load(open(\"tensorflow/configs/mlp.yml\", 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "experiment = MLExperiment(config)\n",
    "\n",
    "print(experiment.config_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start training the model for a given number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2 epochs...\n",
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2923 - accuracy: 0.9160 - val_loss: 0.1488 - val_accuracy: 0.9549\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1548 - accuracy: 0.9560 - val_loss: 0.1170 - val_accuracy: 0.9666\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "experiment.run(epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and restoring an experiment\n",
    "\n",
    "The current experiment's state can been saved into file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_state import Serializer\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# create a temporary directory\n",
    "temp_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "# file that will store the experiment\n",
    "file_path = Path(temp_dir.name) / 'exp.save'\n",
    "\n",
    "# save the experiment using the Pickle serializer\n",
    "Serializer({'class': 'Pickle'}).save(experiment, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment can be restored and resumed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2 epochs...\n",
      "Epoch 3/4\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1261 - accuracy: 0.9648 - val_loss: 0.1055 - val_accuracy: 0.9718\n",
      "Epoch 4/4\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1126 - accuracy: 0.9691 - val_loss: 0.0999 - val_accuracy: 0.9739\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "experiment = Serializer({'class': 'Pickle'}).load(file_path)\n",
    "\n",
    "experiment.run(epochs=2)\n",
    "\n",
    "temp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration grammar\n",
    "\n",
    "We can customize the config and change the different components of the experiment. The set of valid configurations depends on an underlying grammar that is defined uppon the definition of the `ConfigState` objects. In our example, the model is defined using the `Model` class:\n",
    "\n",
    "```python\n",
    "@builder\n",
    "class Model(ConfigState):\n",
    "  input_shape: Tuple[int] = ConfigField(..., \"Input shape of the model\", type=tuple)\n",
    "  output_units: Optional[int] = ConfigField(..., \"Model's output units count\", type=int)\n",
    "    \n",
    "  @abstractmethod\n",
    "  def _build_keras_model(self) -> tf.keras.Model:\n",
    "    \"\"\"Build the keras model\"\"\"\n",
    "```\n",
    "\n",
    "It is an abstract class decorated with the `@builder` decorator meaning that it used as a factory to build concrete subclasses representing the different model architectures that we would like to be able to instanciate. The subclasses decorated with `@register` can be built using the factory, for example we can define a class to build multi layer perceptron models:\n",
    "\n",
    "```python\n",
    "@register\n",
    "class MultiLayerPerceptron(Model):\n",
    "  structure: List[int] = ConfigField([128], \"hidden structure of the MLP\")\n",
    "  dropout_rate: float = ConfigField(0.0, \"Dropout rate applied on the last \"\n",
    "                                         \"hidden layer.\")\n",
    "\n",
    "  def _build_keras_model(self) -> tf.keras.Model:\n",
    "    ...\n",
    "```\n",
    "`MultiLayerPerceptron` have `ConfigField` attributes that define its configuration interface. For instance, `structure` is a config field representing a list of integer that specifies the size and the number of the hidden layers. In the same way, we can define a class to build CNNs:\n",
    "\n",
    "```python\n",
    "@register\n",
    "class CNN(Model):\n",
    "  structure: List[Union[int, str]] = ConfigField([32, 'max', 64, 'max', 64],\n",
    "                                                 \"Convolutional structure. \"\n",
    "                                                 \"Conv2D layers units \"\n",
    "                                                 \"are integers, pooling \"\n",
    "                                                 \"layers type are str among \"\n",
    "                                                 \"'max' or 'average'.\")\n",
    "  def _build_keras_model(self) -> tf.keras.Model:\n",
    "    ...\n",
    "```\n",
    "Now, the grammar for the `structure` configuration field is different: it is a sequence of integers and strings representing `Conv2D` layers units for intergers, while strings represents the pooling layers specified with either 'max' or 'average' for respectively `MaxPooling2D` or `AveragePooling2D` layers. This is a convenient way to represent an alternating sequence of convolutional and pooling layers. It is uppon the developper to design a grammar rich enough to allow a large set of valid configurations.\n",
    "\n",
    "Here are some valid model configurations that can be defined using our grammar:\n",
    "```yaml\n",
    "model:\n",
    "  class: CNN\n",
    "  structure: [32, 32, 'average', 64, 64, 'max', 64, 128]\n",
    "```\n",
    "```yaml\n",
    "model:\n",
    "  class: CNN\n",
    "  structure: [16, 'average', 32, 'max', 64, 128, 'max']\n",
    "```\n",
    "```yaml\n",
    "model:\n",
    "  class: MultiLayerPerceptron\n",
    "  structure: [128, 64]\n",
    "  dropout_rate: 0.5\n",
    "```\n",
    "\n",
    "The other componants of the experiment follow the same princible. As long as the user is able to setup a valid configuration, he can instantiate and run a new experiment with little to no coding skill requirement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  batch_size: 32\n",
      "  name: cifar10\n",
      "model:\n",
      "  class: CNN\n",
      "  input_shape: (32, 32, 3)\n",
      "  output_units: 10\n",
      "  structure: [32, max, 64, max, 64]\n",
      "optimizer:\n",
      "  beta_1: 0.9\n",
      "  beta_2: 0.9999\n",
      "  class: Adam\n",
      "  epsilon: 1e-07\n",
      "  learning_rate: 0.001\n",
      "\n",
      "Training for 20 epochs...\n",
      "Epoch 1/20\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.5197 - accuracy: 0.4497 - val_loss: 1.3030 - val_accuracy: 0.5402\n",
      "Epoch 2/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1959 - accuracy: 0.5785 - val_loss: 1.1783 - val_accuracy: 0.5838\n",
      "Epoch 3/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0583 - accuracy: 0.6297 - val_loss: 1.0382 - val_accuracy: 0.6352\n",
      "Epoch 4/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.9619 - accuracy: 0.6642 - val_loss: 0.9618 - val_accuracy: 0.6705\n",
      "Epoch 5/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8866 - accuracy: 0.6933 - val_loss: 0.9318 - val_accuracy: 0.6809\n",
      "Epoch 6/20\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8329 - accuracy: 0.7112 - val_loss: 0.9537 - val_accuracy: 0.6710\n",
      "Epoch 7/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7910 - accuracy: 0.7252 - val_loss: 0.9180 - val_accuracy: 0.6886\n",
      "Epoch 8/20\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7478 - accuracy: 0.7387 - val_loss: 0.9017 - val_accuracy: 0.6997\n",
      "Epoch 9/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7157 - accuracy: 0.7513 - val_loss: 0.8831 - val_accuracy: 0.7065\n",
      "Epoch 10/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6859 - accuracy: 0.7614 - val_loss: 0.8951 - val_accuracy: 0.7046\n",
      "Epoch 11/20\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6565 - accuracy: 0.7726 - val_loss: 0.9128 - val_accuracy: 0.7012\n",
      "Epoch 12/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6345 - accuracy: 0.7775 - val_loss: 0.9484 - val_accuracy: 0.6947\n",
      "Epoch 13/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6073 - accuracy: 0.7875 - val_loss: 0.9441 - val_accuracy: 0.6990\n",
      "Epoch 14/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.5847 - accuracy: 0.7956 - val_loss: 0.9669 - val_accuracy: 0.7006\n",
      "Epoch 15/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.5608 - accuracy: 0.8033 - val_loss: 0.9788 - val_accuracy: 0.6965\n",
      "Epoch 16/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.5370 - accuracy: 0.8111 - val_loss: 0.9876 - val_accuracy: 0.7029\n",
      "Epoch 17/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.5209 - accuracy: 0.8175 - val_loss: 1.0114 - val_accuracy: 0.6926\n",
      "Epoch 18/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.5013 - accuracy: 0.8232 - val_loss: 1.0445 - val_accuracy: 0.6925\n",
      "Epoch 19/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4819 - accuracy: 0.8289 - val_loss: 1.0988 - val_accuracy: 0.6797\n",
      "Epoch 20/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4623 - accuracy: 0.8360 - val_loss: 1.1237 - val_accuracy: 0.6873\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "\n",
    "config['dataset'] = {\n",
    "    'name': 'cifar10' # https://www.tensorflow.org/datasets/catalog/overview#image_classification\n",
    "}\n",
    "config['model'] = {\n",
    "    'class': 'CNN',\n",
    "    'structure': [32, 'max', 64, 'max', 64]\n",
    "}\n",
    "config['optimizer'] = {\n",
    "    'class': 'Adam',\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "\n",
    "experiment = MLExperiment(config)\n",
    "\n",
    "print(experiment.config_summary())\n",
    "\n",
    "experiment.run(epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration grammar and composability\n",
    "\n",
    "One powerfull feature of `ConfigState` is the ability to compose objects. For instance, we can define a model representing an ensemble of another base model:\n",
    "\n",
    "```python\n",
    "@register\n",
    "class Ensembler(Model):\n",
    "  model: Model = ConfigField(type=Model, doc=\"The model to be ensembled\")\n",
    "  ensemble_size: int = ConfigField(2, \"Size of the ensemble\", force_type=True)\n",
    "  input_shape = ConfigField(model.input_shape)\n",
    "  output_units = ConfigField(model.output_units)\n",
    "\n",
    "  def _build_keras_model(self) -> tf.keras.Model:\n",
    "    ...\n",
    "```\n",
    "\n",
    "In `Ensembler`, the `model` configuration field represents another `Model` to be used as base component for the ensembling. This way, different models can be composed together to represent more complex ones. Notice the `input_shape` and `output_units` fields, they are *references* and enable the configuration settings to be injected into nested `ConfigState` objects. By tying the fields in such a way, this constrains the configuration grammar and reduces its verbosity as well as help preventing inconsistent configurations. The following are valid ensemble models configs:\n",
    "\n",
    "```yaml\n",
    "model:\n",
    "  class: Ensembler\n",
    "  ensemble_size: 4\n",
    "  model:\n",
    "    class: MultiLayerPerceptron\n",
    "    structure: [128]\n",
    "```\n",
    "```yaml\n",
    "model:\n",
    "  class: Ensembler\n",
    "  ensemble_size: 4\n",
    "  model:\n",
    "    class: CNN\n",
    "    structure: [32, 'max', 64, 'max', 64]\n",
    "```\n",
    "\n",
    "Furthermore, since `Ensembler` is itself a `Model`, we can compose it into another `Ensembler` such that we can define models that are ensemble of ensemble of ensemble, etc ...:\n",
    "\n",
    "```yaml\n",
    "model:\n",
    "  class: Ensembler\n",
    "  ensemble_size: 4\n",
    "  model:\n",
    "    class: Ensembler\n",
    "    ensemble_size: 4\n",
    "    model:\n",
    "      class: CNN\n",
    "      structure: [32, 'max', 64, 'max', 64]\n",
    "```\n",
    "\n",
    "Following are examples of configuring and running experiments with ensemble models.\n",
    "\n",
    "***Configuring an ensemble of CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  batch_size: 128\n",
      "  name: cifar10\n",
      "model:\n",
      "  class: Ensembler\n",
      "  ensemble_size: 4\n",
      "  input_shape: (32, 32, 3)\n",
      "  model:\n",
      "    class: CNN\n",
      "    input_shape: (32, 32, 3)\n",
      "    output_units: 10\n",
      "    structure: [32, max, 64, max, 64]\n",
      "  output_units: 10\n",
      "optimizer:\n",
      "  beta_1: 0.9\n",
      "  beta_2: 0.9999\n",
      "  class: Adam\n",
      "  epsilon: 1e-07\n",
      "  learning_rate: 0.001\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 [(None, 32, 32, 3),  0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 10)           66570       lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 10)           66570       lambda[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 10)           66570       lambda[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       (None, 10)           66570       lambda[0][3]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 10)           0           sequential_3[0][0]               \n",
      "                                                                 sequential_4[0][0]               \n",
      "                                                                 sequential_5[0][0]               \n",
      "                                                                 sequential_6[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 266,280\n",
      "Trainable params: 266,280\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'optimizer': {\n",
    "        'class': 'Adam',\n",
    "        'learning_rate': 0.001\n",
    "    },\n",
    "    'dataset': {\n",
    "        'name': 'cifar10',\n",
    "        'batch_size': 128 # We augment the batch_size so that each ensembled models train on batches of 32 elements\n",
    "    }\n",
    "}\n",
    "\n",
    "cnn_model = {\n",
    "    'class': 'CNN',\n",
    "    'structure': [32, 'max', 64, 'max', 64]\n",
    "}\n",
    "\n",
    "config['model'] = {\n",
    "    'class': 'Ensembler',\n",
    "    'model': cnn_model,\n",
    "    'ensemble_size': 4\n",
    "}\n",
    "\n",
    "experiment = MLExperiment(config)\n",
    "\n",
    "print(experiment.config_summary())\n",
    "\n",
    "print(experiment.model.keras_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20 epochs...\n",
      "Epoch 1/20\n",
      "391/391 [==============================] - 4s 8ms/step - loss: 1.8319 - accuracy: 0.3260 - val_loss: 1.5102 - val_accuracy: 0.4517\n",
      "Epoch 2/20\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.5187 - accuracy: 0.4510 - val_loss: 1.3614 - val_accuracy: 0.5108\n",
      "Epoch 3/20\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.3832 - accuracy: 0.5008 - val_loss: 1.2650 - val_accuracy: 0.5440\n",
      "Epoch 4/20\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.2965 - accuracy: 0.5378 - val_loss: 1.1784 - val_accuracy: 0.5758\n",
      "Epoch 5/20\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.2279 - accuracy: 0.5641 - val_loss: 1.1199 - val_accuracy: 0.6020\n",
      "Epoch 6/20\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.1729 - accuracy: 0.5874 - val_loss: 1.0526 - val_accuracy: 0.6325\n",
      "Epoch 7/20\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.1256 - accuracy: 0.6050 - val_loss: 1.0232 - val_accuracy: 0.6410\n",
      "Epoch 8/20\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.0814 - accuracy: 0.6218 - val_loss: 0.9827 - val_accuracy: 0.6590\n",
      "Epoch 9/20\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.0482 - accuracy: 0.6333 - val_loss: 0.9522 - val_accuracy: 0.6706\n",
      "Epoch 10/20\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.0184 - accuracy: 0.6448 - val_loss: 0.9271 - val_accuracy: 0.6789\n",
      "Epoch 11/20\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9857 - accuracy: 0.6565 - val_loss: 0.9158 - val_accuracy: 0.6807\n",
      "Epoch 12/20\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9561 - accuracy: 0.6656 - val_loss: 0.8864 - val_accuracy: 0.6918\n",
      "Epoch 13/20\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9344 - accuracy: 0.6746 - val_loss: 0.8704 - val_accuracy: 0.7008\n",
      "Epoch 14/20\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9155 - accuracy: 0.6817 - val_loss: 0.8512 - val_accuracy: 0.7053\n",
      "Epoch 15/20\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9015 - accuracy: 0.6870 - val_loss: 0.8319 - val_accuracy: 0.7124\n",
      "Epoch 16/20\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8751 - accuracy: 0.6964 - val_loss: 0.8292 - val_accuracy: 0.7169\n",
      "Epoch 17/20\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8590 - accuracy: 0.6998 - val_loss: 0.8115 - val_accuracy: 0.7237\n",
      "Epoch 18/20\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8444 - accuracy: 0.7079 - val_loss: 0.8051 - val_accuracy: 0.7231\n",
      "Epoch 19/20\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8228 - accuracy: 0.7150 - val_loss: 0.8019 - val_accuracy: 0.7248\n",
      "Epoch 20/20\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8157 - accuracy: 0.7179 - val_loss: 0.7750 - val_accuracy: 0.7352\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "experiment.run(epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Configuring an ensemble of ensemble of CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  batch_size: 512\n",
      "  name: cifar10\n",
      "model:\n",
      "  class: Ensembler\n",
      "  ensemble_size: 4\n",
      "  input_shape: (32, 32, 3)\n",
      "  model:\n",
      "    class: Ensembler\n",
      "    ensemble_size: 4\n",
      "    input_shape: (32, 32, 3)\n",
      "    model:\n",
      "      class: CNN\n",
      "      input_shape: (32, 32, 3)\n",
      "      output_units: 10\n",
      "      structure: [32, max, 64, max, 64]\n",
      "    output_units: 10\n",
      "  output_units: 10\n",
      "optimizer:\n",
      "  beta_1: 0.9\n",
      "  beta_2: 0.9999\n",
      "  class: Adam\n",
      "  epsilon: 1e-07\n",
      "  learning_rate: 0.001\n",
      "\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_27 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              [(None, 32, 32, 3),  0           input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Functional)            (None, 10)           266280      lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Functional)            (None, 10)           266280      lambda_10[0][1]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Functional)            (None, 10)           266280      lambda_10[0][2]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_4 (Functional)            (None, 10)           266280      lambda_10[0][3]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 10)           0           model_1[0][0]                    \n",
      "                                                                 model_2[0][0]                    \n",
      "                                                                 model_3[0][0]                    \n",
      "                                                                 model_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,065,120\n",
      "Trainable params: 1,065,120\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'optimizer': {\n",
    "        'class': 'Adam',\n",
    "        'learning_rate': 0.001\n",
    "    },\n",
    "    'dataset': {\n",
    "        'name': 'cifar10',\n",
    "        'batch_size': 512\n",
    "    }\n",
    "}\n",
    "\n",
    "cnn_model = {\n",
    "    'class': 'CNN',\n",
    "    'structure': [32, 'max', 64, 'max', 64]\n",
    "}\n",
    "\n",
    "ensemble = {\n",
    "    'class': 'Ensembler',\n",
    "    'model': cnn_model,\n",
    "    'ensemble_size': 4\n",
    "}\n",
    "\n",
    "config['model'] = {\n",
    "    'class': 'Ensembler',\n",
    "    'model': ensemble,\n",
    "    'ensemble_size': 4\n",
    "}\n",
    "\n",
    "experiment = MLExperiment(config)\n",
    "\n",
    "# ensemble_ensemble_exp.model.model.output_units\n",
    "print(experiment.config_summary())\n",
    "\n",
    "print(experiment.model.keras_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1 epochs...\n",
      "98/98 [==============================] - 5s 32ms/step - loss: 2.1411 - accuracy: 0.1980 - val_loss: 1.8732 - val_accuracy: 0.3440\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "experiment.run(epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}